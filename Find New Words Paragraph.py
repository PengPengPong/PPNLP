#!/usr/bin/env python# encoding=utf-8# 本程序用于抽取一段大文本中，可能为常用的词组组合# 对文本，预先按照标点符号分割后，将分割后的每一段作为独立的句子送入# 由于是分别送入的，因此对文本进行整体处理的方案将无法适用，只能独立处理，独立判断词汇之前是否已经存在。这将影响一些时间效率。# 从之前的运行情况来看，对130W字的西游记，文本整体处理需要花费66s。文本切分，不做排序，花费53s。# 采用迭代器，降低内存的消耗from __future__ import division   # 保证整数相处结果为浮点数import reimport sysimport jsonfrom time import timeimport mathimport hashlibimport fnmatchimport os# 注意加上繁简转化inpath = "/Users/pp/pycharmprojects/nlp/jobtitle/alpha_jobtitle.csv"uipath = unicode(inpath, "utf8")reload(sys)sys.setdefaultencoding('utf8')def SegSentense(String): # 将一大段文本，根据标点符号拆分成一个一个的句子    s=String.decode('utf-8')    text = re.sub("[\d\s+\.\!\/_,$%^*(+\"\']+|[+——！，{。？、<;>《》“”【】\[\]：·:；~@#￥%……&*（\）)]+".decode("utf8"),"\t".decode("utf8"),s)    return textdef CleanPunc(String): # 删除所有非空格的标点符号以及数字，含中英文    s=String.decode('utf-8')    text = re.sub("[\d\s+\.\!\/_,$%^*(+\"\']+|[+——！，{。？、<;>《》“”【】\[\]：·:；~@#￥%……&*（\）)]+".decode("utf8"),"".decode("utf8"),s)    return textclass vocabulary():    words=set()    def __init__(self,word):        self.word=word        self.freq=0        self.left_words=[]        self.right_words=[]        self.left_entropy=0        self.right_entropy=0        self.coagulation=0    def CaculateEntropy(self):        entropy_words=[self.right_words,self.left_words]        entlist = []        for textlist in entropy_words:            l=len(textlist)            ent = 0            dic = {}            for i in textlist:                dic[i]=dic.get(i,0)+1            for i in dic.keys():                p=dic[i]/l                ent-=p*math.log(p)            entlist.append(ent)        return min(entlist)    def CaculateCoagulation(self,TotalTextLength,Window_Min):        if len(self.word)-1<Window_Min:            self.coagulation=1000            return self.coagulation        # print 'selfword',self.word        textposible=TextGroup(self.word,len(self.word)-1,Window_Min)        posibility=[]        # print 'textposible',json.dumps(textposible,ensure_ascii=False)        for text in textposible:            # print 'text',json.dumps(text,ensure_ascii=False)            totalposibility=1            for textseg in text:                # print 'textseg',textseg                totalposibility*=objlist[objnamedic[textseg]].freq/TotalTextLength                # print 'totalposibility',totalposibility            posibility.append(totalposibility)        max_posibility=max(posibility)        # print 'minposibility',min_posibility        self.coagulation=self.freq/TotalTextLength/max_posibility        return self.coagulationclass MySentences(object):   # 创建一个迭代器，往模型中送入处理后的分段文本    def __init__(self, dirname):        self.dirname = dirname    def __iter__(self):        j=0        for root, dirs, files in os.walk(self.dirname):            for filename in files:                if not fnmatch.fnmatch(filename,'*.csv'):                    continue                file_path = os.path.join(root,filename)                for line in open(file_path):                    sline = line.strip()   # 判断空行，如果为空行则执行下一次循环                    if sline == "":                        continue                    listline=line.lower().split(',')[1:]   # 全部转化为小写，且将一行的句子拆分为词的列表                    # j += 1                    # if j > 100000:                    #     break                    for text in listline:                        text=CleanPunc(text)                        yield textdef TextSeg(Text,Window_Max,Window_Min):  # 按照最长和最短窗口，生成所有可能文本片段    # Window_Max 最长成词窗口    # Window_Min 最短成词窗口    l = len(Text)    text_seg = []    for i in range(l):        for j in range(Window_Max-2):            begin = l - i - j - Window_Min            if begin >= 0:                segments = Text[begin:l - i]  # 按照窗口，截取文本片段            else:                continue            text_seg.append(segments)    return text_segresult=[]temp=[]def TextGroup(Text,Window_Max,Window_Min):  # 利用递归来实现    # Window_Max 最长成词窗口    # Window_Min 最短成词窗口    global temp    global result    # 如果Window_Max大于文本长度，则主动降低Window_Max，以免无畏计算    if Window_Max>len(Text):        Window_Max=len(Text)    if len(Text)==0:                    # 如果直接正好切分到没有词，则直接记入        temp.append(Text)        # print 'temp00',temp        result.append(temp[:-1])        return None    if len(Text)<Window_Min:        temp.append(Text)        # print 'temp01', temp        return None    if len(Text)<2*Window_Min:        temp.append(Text)        # print Text        # print 'temp02', temp        result.append(temp)     # 触达边界后，一次搜索结束，判断结果是否合适，合适则记入        return result    for i in range(Window_Min,Window_Max+1):        temp.append(Text[:i])   # 这个是经过的节点路径        # print 'temp1',temp        New_Text = Text[i:]        TextGroup(New_Text, Window_Max, Window_Min)  # 基于当前节点状态继续深入搜索        # print result        if len(''.join(temp))==len(''.join(result[0])):  # 如果是叶子节点，返回两层            temp = temp[:-2]    # 搜索完毕之后，状态要回到上一个节点。如果不是根节点，则回两层。        else:            temp = temp[:-1]    # 搜索完毕之后，状态要回到上一个节点。如果是根节点，则只返回一层        # print 'temp2', temp    return resultobjlist=[]objnamedic={}index=0def FindNewWords_Paragraph(RawTextList,Window_Max,Window_Min,TotalTextLength,MinFreq=10,MinEnt=2,MinCoaulation=100):  # 此函数要求送入的是一个文本列表，列表中每个元素就是一句话    start = time()    newwords=[]    global objlist    global objnamedic    global index    global result    global temp    count=0    for RawText in RawTextList:        count+=1        if count%10000==0:            print 'processed setup',count/jobtitlenum,'total cost',time()-start        text_raw='B'+RawText.decode('utf8')+'E' # 处理头尾部丢失的问题，加一个头、尾字符。不然第一个和最后一个词无法提取        text_seg=TextSeg(text_raw,Window_Max+2,Window_Min+2)  # 由于要同时取出左邻词和右邻词，因此，在取出来的字符串抛去两个才是想要的词        name=locals()        for i in text_seg:            # print 'text seg',i            main_word = i[1:-1]            # print 'main word', main_word            right_word = i[-1]            left_word = i[0]            # objname = hashlib.md5(main_word.encode("GB18030")).hexdigest()            objname = hashlib.md5(main_word.encode("utf8")).hexdigest()            if main_word not in vocabulary.words:  # in set 的时间复杂度，平均为O(1),最差为O(n)                # print 'main word',main_word                vocabulary.words.add(main_word)                name[objname] = vocabulary(main_word)                objlist.append(name[objname])                objnamedic.setdefault(main_word, index)                index += 1            name[objname].freq += 1            name[objname].right_words.append(right_word)            name[objname].left_words.append(left_word)    count = 0    print 'start to caculate total vac'    totalobjnum=len(objlist)    print 'total vac',totalobjnum    print 'start to caculate entropy/coagulation'    for i in objlist:        result = []        temp = []        count+=1        if count==1:            print 'I start to caculate'        if count%10000==0:            print 'processed caculate',count/totalobjnum,'total cost',time()-start        entropy=i.CaculateEntropy()        coagulation=i.CaculateCoagulation(TotalTextLength,Window_Min)        if entropy>MinEnt and coagulation>MinCoaulation and len(i.word)>Window_Min+1 and i.freq>MinFreq:            newwords.append([i.word,i.freq,entropy,coagulation])    sortcosttime=time()-start    print 'sort cost %d'% sortcosttime    return newwordsobjlist=[]objnamedic={}index=0def FindNewWords(RawText,Window_Max,Window_Min,TotalTextLength,MinFreq=10,MinEnt=2,MinCoaulation=100): # 此函数要求送入的是一个整段的文本    start = time()    newwords=[]    global objlist    global objnamedic    global index    global result    global temp    text_raw=RawText.decode('utf8')+'E' # 处理尾部丢失的问题，加一个结尾字符。不然最后一个词无法提取    text_seg=TextSeg(text_raw,Window_Max+1,Window_Min+1) # 由于要从尾部抽取一个字作为右邻词，因此，整体的词的切分上，需要比要求额外多一个字    text_seg.sort(key=lambda x:x[:-1])  # 预先去掉后缀后，按照字母表排序    name=locals()    for j in range(len(text_seg)):        text=text_seg[j]   # 取出预处理的词串        main_word=text[:-1]   # 取出预处理的单词        try:            pre_main_word=text_seg[j-1][:-1]        except:            pre_main_word=''        right_word=text[-1]        if main_word<>pre_main_word:            # print 'main_word',main_word            # objname = hashlib.md5(main_word.encode("GB18030")).hexdigest()            objname = hashlib.md5(main_word.encode("utf8")).hexdigest()            name[objname]=vocabulary(main_word)            objlist.append(name[objname])            objnamedic.setdefault(main_word,index)            index += 1        name[objname].freq+=1        name[objname].right_words.append(right_word)    # 将原有文本逆序，用同样的方法取左临词    text_reverse=RawText.decode('utf8')[::-1]    text_reverse=text_reverse+'B' # 处理头部丢失的问题，加一个头部字符。不然最后一个词无法提取    text_seg=TextSeg(text_reverse,Window_Max+1,Window_Min+1)    for j in range(len(text_seg)):        text = text_seg[j]        main_word = text[:-1]        try:            pre_main_word = text_seg[j - 1][:-1]        except:            pre_main_word = ''        left_word = text[-1]        # objname = hashlib.md5(main_word[::-1].encode("GB18030")).hexdigest()        objname = hashlib.md5(main_word[::-1].encode("utf8")).hexdigest()        name[objname].left_words.append(left_word)    for i in objlist:        result = []        temp = []        entropy=i.CaculateEntropy()        coagulation=i.CaculateCoagulation(TotalTextLength,Window_Min)        if entropy > MinEnt and coagulation > MinCoaulation and len(i.word) > Window_Min and i.freq > MinFreq:            newwords.append([i.word,i.freq,entropy,coagulation])    sortcosttime=time()-start    print 'sort cost %d'% sortcosttime    return newwords# 未处理标点def FindNewWordsInJob():    global jobtitlenum    inpath = "/Users/pp/pycharmprojects/nlp/jobtitle"    uipath = unicode(inpath, "utf8")    print 'start to load data'    text=MySentences(inpath)    l=len(''.join(text))    print l    jobtitlenum=sum(1 for _ in text)    print jobtitlenum    # with open(inpath) as f:    #     text=f.read()    print 'start to clean punc'    # text=text.replace('\n','')    # l=len(text)    # text=CleanPunc(text)    # text=text.split('\t')    # i=0    # while i<100:    #     print text[i]    #     i+=1    a=0    print 'start to find new words'    newwords=FindNewWords_Paragraph(text,5,1,l,10,0,10)    print 'start to sort'    newwords.sort(key=lambda x:(x[1],x[3],x[2]),reverse=True)    print 'total words',len(vocabulary.words)    print 'start to write to files'    with open('/Users/pp/pycharmprojects/nlp/new_words_IFC&Alpha_Freq>10&Ent>0&Coagu>10.txt', 'a') as f:        for word, freq, entropy, coagulation in newwords:            # print word,freq,entropy,coagulation            f.write(word+'\t'+str(freq)+'\t'+str(entropy)+'\t'+str(coagulation)+'\n')FindNewWordsInJob()print 'hahahahahah'